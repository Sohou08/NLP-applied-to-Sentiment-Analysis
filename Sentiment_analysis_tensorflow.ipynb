{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pratice example of NLP applied to sentimental analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Downloaded from [kaggle](https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format/data). We have only focus on the train and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the directory \n",
    "X= pd.read_csv('./Train.csv', header = 0)\n",
    "Y= pd.read_csv('./Test.csv', header = 0)\n",
    "##This below dataset combine all data of the train and test table. \n",
    "#That will be useful for generating the vocabulary\n",
    "data_text = pd.read_csv ('./data_text.csv',  header =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "5000\n",
      "45000\n"
     ]
    }
   ],
   "source": [
    "print(len(X))\n",
    "print(len(Y))\n",
    "print(len(data_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.iloc[:, 0]\n",
    "Y_train = X.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = Y.iloc[:, 0]\n",
    "Y_test = Y.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_text = df.iloc[0].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = np.array(data_text)\n",
    "data_text = df.iloc[0].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing raw data\n",
    "\n",
    "As mentioned in the section ([Understand RNN](https://github.com/Sohou08/NLP-applied-to-Sentiment-Analysis/blob/master/Understand%20RNN.md\n",
    ")), the network can not work direclty on test-strings. The first step is to convert word to interger. The process is called in our flowchart 'Tokenizer'. By choice, we are construct the tokenizer on  10000 most popular words from the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_words is None:\n",
    "    num_words = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'of': 3,\n",
       " 'i': 4,\n",
       " 'and': 5,\n",
       " 'thunderbirds': 6,\n",
       " 'school': 7,\n",
       " 'to': 8,\n",
       " 'be': 9,\n",
       " 'was': 10,\n",
       " 'his': 11,\n",
       " 'all': 12,\n",
       " 'my': 13,\n",
       " 'we': 14,\n",
       " 'wanted': 15,\n",
       " 'one': 16,\n",
       " 'with': 17,\n",
       " 'sapiens': 18,\n",
       " 'grew': 19,\n",
       " 'up': 20,\n",
       " 'b': 21,\n",
       " '1965': 22,\n",
       " 'watching': 23,\n",
       " 'loving': 24,\n",
       " 'mates': 25,\n",
       " 'at': 26,\n",
       " 'watched': 27,\n",
       " 'played': 28,\n",
       " 'before': 29,\n",
       " 'during': 30,\n",
       " 'lunch': 31,\n",
       " 'after': 32,\n",
       " 'virgil': 33,\n",
       " 'or': 34,\n",
       " 'scott': 35,\n",
       " 'no': 36,\n",
       " 'alan': 37,\n",
       " 'counting': 38,\n",
       " 'down': 39,\n",
       " 'from': 40,\n",
       " '5': 41,\n",
       " 'became': 42,\n",
       " 'an': 43,\n",
       " 'art': 44,\n",
       " 'form': 45,\n",
       " 'took': 46,\n",
       " 'children': 47,\n",
       " 'see': 48,\n",
       " 'movie': 49,\n",
       " 'hoping': 50,\n",
       " 'they': 51,\n",
       " 'would': 52,\n",
       " 'get': 53,\n",
       " 'glimpse': 54,\n",
       " 'what': 55,\n",
       " 'loved': 56,\n",
       " 'as': 57,\n",
       " 'child': 58,\n",
       " 'how': 59,\n",
       " 'bitterly': 60,\n",
       " 'disappointing': 61,\n",
       " 'only': 62,\n",
       " 'high': 63,\n",
       " 'point': 64,\n",
       " 'snappy': 65,\n",
       " 'theme': 66,\n",
       " 'tune': 67,\n",
       " 'not': 68,\n",
       " 'that': 69,\n",
       " 'it': 70,\n",
       " 'could': 71,\n",
       " 'compare': 72,\n",
       " 'original': 73,\n",
       " 'score': 74,\n",
       " 'thankfully': 75,\n",
       " 'early': 76,\n",
       " 'saturday': 77,\n",
       " 'mornings': 78,\n",
       " 'television': 79,\n",
       " 'channel': 80,\n",
       " 'still': 81,\n",
       " 'plays': 82,\n",
       " 'reruns': 83,\n",
       " 'series': 84,\n",
       " 'gerry': 85,\n",
       " 'anderson': 86,\n",
       " 'wife': 87,\n",
       " 'created': 88,\n",
       " 'jonatha': 89,\n",
       " 'frakes': 90,\n",
       " 'should': 91,\n",
       " 'hand': 92,\n",
       " 'in': 93,\n",
       " 'directors': 94,\n",
       " 'chair': 95,\n",
       " 'version': 96,\n",
       " 'completely': 97,\n",
       " 'hopeless': 98,\n",
       " 'waste': 99,\n",
       " 'film': 100,\n",
       " 'utter': 101,\n",
       " 'rubbish': 102,\n",
       " 'cgi': 103,\n",
       " 'remake': 104,\n",
       " 'may': 105,\n",
       " 'acceptable': 106,\n",
       " 'but': 107,\n",
       " 'replacing': 108,\n",
       " 'marionettes': 109,\n",
       " 'homo': 110,\n",
       " 'subsp': 111,\n",
       " 'huge': 112,\n",
       " 'error': 113,\n",
       " 'judgment': 114,\n",
       " '0': 115}"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer will be used to convert all texts in the training and test set to lists of these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When I put this movie in my DVD player, and sat down with a coke and some chips, I had some expectations. I was hoping that this movie would contain some of the strong-points of the first movie: Awsome animation, good flowing story, excellent voice cast, funny comedy and a kick-ass soundtrack. But, to my disappointment, not any of this is to be found in Atlantis: Milo's Return. Had I read some reviews first, I might not have been so let down. The following paragraph will be directed to those who have seen the first movie, and who enjoyed it primarily for the points mentioned.<br /><br />When the first scene appears, your in for a shock if you just picked Atlantis: Milo's Return from the display-case at your local videoshop (or whatever), and had the expectations I had. The music feels as a bad imitation of the first movie, and the voice cast has been replaced by a not so fitting one. (With the exception of a few characters, like the voice of Sweet). The actual drawings isnt that bad, but the animation in particular is a sad sight. The storyline is also pretty weak, as its more like three episodes of Schooby-Doo than the single adventurous story we got the last time. But dont misunderstand, it's not very good Schooby-Doo episodes. I didnt laugh a single time, although I might have sniggered once or twice.<br /><br />To the audience who haven't seen the first movie, or don't especially care for a similar sequel, here is a fast review of this movie as a stand-alone product: If you liked schooby-doo, you might like this movie. If you didn't, you could still enjoy this movie if you have nothing else to do. And I suspect it might be a good kids movie, but I wouldn't know. It might have been better if Milo's Return had been a three-episode series on a cartoon channel, or on breakfast TV.\""
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  49,  93,  13,   5,  39,  17,   2,   5,   4,   4,  10,  50,\n",
       "        69,  49,  52,   3,   1,   3,   1,  49,   5,   2, 107,   8,  13,\n",
       "        68,   3,   8,   9,  93,   4,   4,  68,  39,   1,   9,   8,   1,\n",
       "        49,   5,  70,   1,   1,  93,   2,  40,   1,  26,  34,   5,   1,\n",
       "         4,   1,  57,   2,   3,   1,  49,   5,   1,   2,  68,  16,  17,\n",
       "         1,   3,   2,   1,   3,   1,  69, 107,   1,  93,   2,   1,  57,\n",
       "         3,   1,  14,   1, 107,  68,   4,   2,   4,  34,   8,   1,   1,\n",
       "        49,  34,   2,   2,   3,  49,  57,   2,  49,  71,  81,  49,   8,\n",
       "         5,   4,  70,   9,   2,  49, 107,   4,  70,   2,  84,   2,  80,\n",
       "        34])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and Truncating Data\n",
    "\n",
    "\n",
    "The Recurrent Neural Network can take sequences of arbitrary length as input, but in order to use a whole batch of data, the sequences need to have the same length. There are two ways to do that: either ensure that all sequences in the entire data-set have the same length, or write a custom data-generator that ensures the sequences have the same length within each batch.\n",
    "The first solution seems more simpler. But if we use the length of the longest sequence in the data-set, then we are wasting a lot of memory. This is particularly important for larger data-sets.\n",
    "So in order to make a compromise, we will use a sequence-length that covers most sequences in the data-set, and we will then truncate longer sequences and pad shorter sequences.\n",
    "First we count the number of tokens in all the sequences in the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in X_train_tokens + X_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.78204444444444"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average number of tokens in a sequence \n",
    "np.mean(num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "803"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximum number of tokens in a sequence\n",
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The max number of tokens we will allow is set to the average plus 2 standard deviations.\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9455333333333333"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This covers about 95% of the data-set.\n",
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When padding or truncating the sequences that have a different length, we need to determine if we want to do this padding or truncating 'pre' or 'post'. If a sequence is truncated, it means that a part of the sequence is simply thrown away. If a sequence is padded, it means that zeros are added to the sequence.\n",
    "So the choice of 'pre' or 'post' can be important because it determines whether we throw away the first or last part of a sequence when truncating, and it determines whether we add zeros to the beginning or end of the sequence when padding. This may confuse the Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pad = pad_sequences(X_test_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 193)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have now transformed the training-set into one big matrix of integers (tokens) with this shape:\n",
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 193)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,  49,  93,  13,   5,  39,  17,   2,   5,   4,   4,  10,  50,\n",
       "        69,  49,  52,   3,   1,   3,   1,  49,   5,   2, 107,   8,  13,\n",
       "        68,   3,   8,   9,  93,   4,   4,  68,  39,   1,   9,   8,   1,\n",
       "        49,   5,  70,   1,   1,  93,   2,  40,   1,  26,  34,   5,   1,\n",
       "         4,   1,  57,   2,   3,   1,  49,   5,   1,   2,  68,  16,  17,\n",
       "         1,   3,   2,   1,   3,   1,  69, 107,   1,  93,   2,   1,  57,\n",
       "         3,   1,  14,   1, 107,  68,   4,   2,   4,  34,   8,   1,   1,\n",
       "        49,  34,   2,   2,   3,  49,  57,   2,  49,  71,  81,  49,   8,\n",
       "         5,   4,  70,   9,   2,  49, 107,   4,  70,   2,  84,   2,  80,\n",
       "        34])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For example, we had the following sequence of tokens above:\n",
    "np.array(X_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Inverse Map\n",
    "Keras implementation of a tokenizer does not seem to have the inverse mapping from integer-tokens back to words, which is needed to reconstruct text-strings from lists of tokens. We have to do by ourself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When I put this movie in my DVD player, and sat down with a coke and some chips, I had some expectations. I was hoping that this movie would contain some of the strong-points of the first movie: Awsome animation, good flowing story, excellent voice cast, funny comedy and a kick-ass soundtrack. But, to my disappointment, not any of this is to be found in Atlantis: Milo's Return. Had I read some reviews first, I might not have been so let down. The following paragraph will be directed to those who have seen the first movie, and who enjoyed it primarily for the points mentioned.<br /><br />When the first scene appears, your in for a shock if you just picked Atlantis: Milo's Return from the display-case at your local videoshop (or whatever), and had the expectations I had. The music feels as a bad imitation of the first movie, and the voice cast has been replaced by a not so fitting one. (With the exception of a few characters, like the voice of Sweet). The actual drawings isnt that bad, but the animation in particular is a sad sight. The storyline is also pretty weak, as its more like three episodes of Schooby-Doo than the single adventurous story we got the last time. But dont misunderstand, it's not very good Schooby-Doo episodes. I didnt laugh a single time, although I might have sniggered once or twice.<br /><br />To the audience who haven't seen the first movie, or don't especially care for a similar sequel, here is a fast review of this movie as a stand-alone product: If you liked schooby-doo, you might like this movie. If you didn't, you could still enjoy this movie if you have nothing else to do. And I suspect it might be a good kids movie, but I wouldn't know. It might have been better if Milo's Return had been a three-episode series on a cartoon channel, or on breakfast TV.\""
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "\n",
    "#Helper-function for converting a list of tokens back to a string of words.\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i movie in my and down with a and i i was hoping that movie would of the of the movie and a but to my not of to be in i i not down the be to the movie and it the the in a from the at or and the i the as a of the movie and the a not one with the of a the of the that but the in a the as of the we the but not i a i or to the the movie or a a of movie as a movie could still movie to and i it be a movie but i it a series a channel or'"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(X_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Recurrent Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer_embedding (Embedding)  (None, 193, 8)            80000     \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 193, 16)           1248      \n",
      "_________________________________________________________________\n",
      "gru_16 (GRU)                 (None, 193, 8)            624       \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 4)                 168       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 82,045\n",
      "Trainable params: 82,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 8\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='layer_embedding'))\n",
    "\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "\n",
    "model.add(GRU(units=4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "594/594 [==============================] - 344s 580ms/step - loss: 0.6289 - accuracy: 0.6376 - val_loss: 0.5995 - val_accuracy: 0.6860\n",
      "Epoch 2/3\n",
      "594/594 [==============================] - 326s 550ms/step - loss: 0.5924 - accuracy: 0.6829 - val_loss: 0.5870 - val_accuracy: 0.6885\n",
      "Epoch 3/3\n",
      "594/594 [==============================] - 320s 539ms/step - loss: 0.5834 - accuracy: 0.6872 - val_loss: 0.5855 - val_accuracy: 0.6790\n",
      "Wall time: 16min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a7b8b2ee48>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X_train_pad, Y_train,\n",
    "          validation_split=0.05, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pad= np.asarray(X_test_pad).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 13s 80ms/step - loss: 0.5824 - accuracy: 0.6926\n"
     ]
    }
   ],
   "source": [
    "#performance of the tests\n",
    "\n",
    "#%%time\n",
    "result = model.evaluate(X_test_pad, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.26%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
